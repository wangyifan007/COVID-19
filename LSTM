import sys
import numpy as np
import pandas as pd
import csv 
import random
import re
import jieba as jb
from keras.preprocessing.text import Tokenizer
from keras import Sequential
from keras import optimizers
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from keras.layers import LSTM, Dense, Bidirectional, Input,Dropout,BatchNormalization,CuDNNLSTM, GRU, CuDNNGRU, Embedding, GlobalMaxPooling1D, GlobalAveragePooling1D, Flatten
from keras.layers import Dense, Embedding, Input, concatenate, Flatten, SpatialDropout1D
from sklearn.model_selection import train_test_split
from keras.models import Model
from keras.preprocessing import text, sequence
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard, LearningRateScheduler, Callback
from keras.optimizers import Adam, Adadelta, SGD, RMSprop, Nadam
from keras import backend as K
f1=open(r'contentemotion.csv', encoding='utf_8_sig',errors = 'ignore') 
df = pd.read_csv(f1) 
print(df.head())
df=df[['content','emotionsocre']]
def remove_punctuation(line):
    line = str(line)
    if line.strip()=='':
        return ''
    rule = re.compile(u"[^a-zA-Z0-9\u4E00-\u9FA5]")
    line = rule.sub('',line)
    return line
 
def stopwordslist(filepath):  
    stopwords = [line.strip() for line in open(filepath, 'r').readlines()]  
    return stopwords  
stopwords = stopwordslist("stopword1.txt")
df['clean_review'] = df['content'].apply(remove_punctuation)
df['clean_review'][:5]
df['cut_review'] = df['clean_review'].apply(lambda x: " ".join([w for w in list(jb.cut(x)) if w not in stopwords]))
df['cut_review'][:5]
MAX_NB_WORDS = 50000
MAX_SEQUENCE_LENGTH = 250
EMBEDDING_DIM = 100

tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(df['cut_review'].values)
word_index = tokenizer.word_index
print(len(word_index))
X = tokenizer.texts_to_sequences(df['cut_review'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
#Y = pd.get_dummies(df['cat_id']).values
Y = pd.get_dummies(df['emotionsocre']).values
x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.08, random_state = 42)
model = Sequential()
model.add(Embedding(MAX_NB_WORDS, 20, input_length=X.shape[1]))
#model.add(SpatialDropout1D(0.2))
#model.add(LSTM(10, dropout=0.2))
model.add(LSTM(100,dropout=0.2))
model.add(Dense(3, activation='softmax'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs =4
batch_size = 64
 
history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,
                    callbacks=[])
score, acc = model.evaluate(x_test, y_test,
                            batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)
def predict(text):
    txt = remove_punctuation(text)
    txt = [" ".join([w for w in list(jb.cut(txt)) if w not in stopwords])]
    seq = tokenizer.texts_to_sequences(txt)
    padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
    pred = model.predict(padded)
    cat_id= pred.argmax(axis=1)[0]
    return cat_id
f1=open(r'nCoV_900k_train.unlabled.csv', encoding='utf_8_sig',errors = 'ignore')
data1 = pd.read_csv(f1)
print(data1.head())
for i in range(0,len(data1)):
    a=data1.iloc[i:i+1,:]
    a=np.array(a).reshape(1,-1)[0].tolist()
    yuce=predict(a[3])
    if(yuce==0):
        yuce=-1
    elif(yuce==1):
        yuce=0
    else:
        yuce=1
    with open(r'nCoV_900k_train.unlabled.csv',  'a', encoding='utf_8_sig', newline='') as f2:
            writer = csv.writer(f2)
            a.append(yuce)
            writer.writerow(a)
